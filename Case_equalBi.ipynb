{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /usr/local/lib/python3.11/site-packages (2.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install pyspark\n",
    "%pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.google.cloud.spark#spark-bigquery-with-dependencies_2.12 added as a dependency\n",
      "com.google.cloud.bigdataoss#gcs-connector added as a dependency\n",
      "com.google.guava#guava added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-469dc1a7-a469-46a6-893b-5e0ad620ce1e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.22.0 in central\n",
      "\tfound com.google.cloud.bigdataoss#gcs-connector;hadoop3-1.9.5 in central\n",
      "\tfound com.google.api-client#google-api-client-java6;1.24.1 in central\n",
      "\tfound com.google.api-client#google-api-client;1.24.1 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.24.1 in central\n",
      "\tfound com.google.http-client#google-http-client;1.24.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.0.1 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.0.1 in central\n",
      "\tfound commons-logging#commons-logging;1.1.1 in central\n",
      "\tfound commons-codec#commons-codec;1.6 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.24.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.9.2 in central\n",
      "\tfound com.google.guava#guava;26.0-jre in central\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.1.3 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.14 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client-java6;1.24.1 in central\n",
      "\tfound com.google.api-client#google-api-client-jackson2;1.24.1 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev135-1.24.1 in central\n",
      "\tfound com.google.cloud.bigdataoss#util;1.9.5 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.6.2 in central\n",
      "\tfound com.google.cloud.bigdataoss#util-hadoop;hadoop3-1.9.5 in central\n",
      "\tfound com.google.cloud.bigdataoss#gcsio;1.9.5 in central\n",
      ":: resolution report :: resolve 1000ms :: artifacts dl 84ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.9.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;1.24.1 from central in [default]\n",
      "\tcom.google.api-client#google-api-client-jackson2;1.24.1 from central in [default]\n",
      "\tcom.google.api-client#google-api-client-java6;1.24.1 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev135-1.24.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.6.2 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcs-connector;hadoop3-1.9.5 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcsio;1.9.5 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#util;1.9.5 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#util-hadoop;hadoop3-1.9.5 from central in [default]\n",
      "\tcom.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.22.0 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.1.3 from central in [default]\n",
      "\tcom.google.guava#guava;26.0-jre from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.24.1 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.24.1 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.24.1 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client-java6;1.24.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.6 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.0.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.0.1 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.14 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.guava#guava;r05 by [com.google.guava#guava;26.0-jre] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   26  |   0   |   0   |   1   ||   25  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-469dc1a7-a469-46a6-893b-5e0ad620ce1e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 25 already retrieved (0kB/81ms)\n",
      "24/07/16 22:56:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import  StringType, IntegerType, ShortType, DateType\n",
    "from typing import List\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Inicialize o SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL NortWind\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config('spark.jars.packages','com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.22.0,com.google.cloud.bigdataoss:gcs-connector:hadoop3-1.9.5,com.google.guava:guava:r05')\\\n",
    "    .config(\"spark.jars\", \"/home/notebooks/CaseEqualBi/postgresql-42.7.3.jar\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracao tabelas Banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 22:56:28,759 - __main__ - INFO - Início do processo ETL\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Início do processo ETL\")\n",
    "import json \n",
    "import os\n",
    "\n",
    "with open(\"config.json\", \"r\") as file: \n",
    "    config = json.load(file)\n",
    "\n",
    "db_config = config['database']\n",
    "url_DB = db_config['url_DB']\n",
    "properties = {\n",
    "    \"user\": \"leotnunesn\",\n",
    "    \"password\": os.getenv('POSTGRES_PASSWORD'),\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "def status_conexaoDB():\n",
    "    try:\n",
    "        spark.read.jdbc(url=url_DB, table=\"categories\", properties=properties) \n",
    "        logger.info(\"Conexão com DB bem sucedida!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Falha na conexão: {e}\") \n",
    "        logger.info(f\"Falha na conexão: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 22:56:35,712 - __main__ - INFO - Conexão com DB bem sucedida!\n",
      "2024-07-16 22:56:38,192 - __main__ - INFO - COLUNA: categories # CARREGADA COM SUCESSO #\n",
      "2024-07-16 22:56:38,520 - __main__ - INFO - customer_customer_demo # NÃO CARREGADA POR SER TABELA VAZIA #\n",
      "2024-07-16 22:56:38,823 - __main__ - INFO - customer_demographics # NÃO CARREGADA POR SER TABELA VAZIA #\n",
      "2024-07-16 22:56:39,094 - __main__ - INFO - COLUNA: customers # CARREGADA COM SUCESSO #\n",
      "2024-07-16 22:56:39,361 - __main__ - INFO - COLUNA: employee_territories # CARREGADA COM SUCESSO #\n",
      "2024-07-16 22:56:39,626 - __main__ - INFO - COLUNA: employees # CARREGADA COM SUCESSO #\n",
      "2024-07-16 22:56:39,921 - __main__ - INFO - COLUNA: orders # CARREGADA COM SUCESSO #\n",
      "2024-07-16 22:56:40,207 - __main__ - INFO - COLUNA: products # CARREGADA COM SUCESSO #\n",
      "2024-07-16 22:56:40,449 - __main__ - INFO - COLUNA: region # CARREGADA COM SUCESSO #\n",
      "2024-07-16 22:56:40,693 - __main__ - INFO - COLUNA: shippers # CARREGADA COM SUCESSO #\n",
      "2024-07-16 22:56:41,008 - __main__ - INFO - COLUNA: suppliers # CARREGADA COM SUCESSO #\n",
      "2024-07-16 22:56:41,264 - __main__ - INFO - COLUNA: territories # CARREGADA COM SUCESSO #\n",
      "2024-07-16 22:56:41,505 - __main__ - INFO - COLUNA: us_states # CARREGADA COM SUCESSO #\n",
      "2024-07-16 22:56:41,505 - __main__ - INFO - Tabelas carregadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "if status_conexaoDB():\n",
    "    class TabelaSpark:\n",
    "        def __init__(self, nome: str, dataframe: DataFrame):\n",
    "            self.nome = nome\n",
    "            self.dataframe = dataframe\n",
    "\n",
    "    lista_tabelas = [\"categories\", \"customer_customer_demo\", \n",
    "                    \"customer_demographics\", \"customers\", \n",
    "                    \"employee_territories\", \n",
    "                    \"employees\", \"orders\", \n",
    "                    \"products\", \"region\", \n",
    "                    \"shippers\", \"suppliers\", \n",
    "                    \"territories\",\"us_states\"] \n",
    "\n",
    "    tabelasspark:  List[TabelaSpark] = []\n",
    "\n",
    "    for nometabela in lista_tabelas:  \n",
    "        df = spark.read.jdbc(url=url_DB, table=nometabela, properties=properties) \n",
    "        if df.isEmpty(): \n",
    "            logger.info(f\"{nometabela} # NÃO CARREGADA POR SER TABELA VAZIA #\")\n",
    "        else:   \n",
    "            sparktables = TabelaSpark(nometabela, dataframe=df) \n",
    "            tabelasspark.append(sparktables) \n",
    "            logger.info(f\"COLUNA: {nometabela} # CARREGADA COM SUCESSO #\")\n",
    "    logger.info(\"Tabelas carregadas com sucesso!\")\n",
    "else:\n",
    "    logger.info(\"Erro na conexão. Verifique suas configurações e tente novamente.\") \n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracao tabela csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 22:57:12,991 - __main__ - INFO - Conexão com drive bem sucedida!\n",
      "2024-07-16 22:57:12,992 - __main__ - INFO - Dados do csv extraidos para variavel: df!\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles \n",
    "import requests\n",
    "import io\n",
    "\n",
    "with open(\"config.json\", \"r\") as file: \n",
    "    config = json.load(file) \n",
    "\n",
    "file_config = config['files']\n",
    "fileid = file_config['fileid']\n",
    "url_csv = file_config['url_csv']\n",
    "localfile = file_config['localfile']\n",
    "\n",
    "response = requests.get(url_csv)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    df = io.BytesIO(response.content)\n",
    "    #Caso o sparkContext fosse a proprio notebook\n",
    "    # with open(\"data.csv\", 'wb') as f:\n",
    "    #     f.write(response.content)\n",
    "    logger.info(\"Conexão com drive bem sucedida!\")\n",
    "    logger.info(\"Dados do csv extraidos para variavel: df!\")\n",
    "else:\n",
    "    logger.info(f\"Erro ao baixar o arquivo: {response.status_code} - {response.reason}\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df_pandas = pd.read_csv(df) \n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "#Caso o sparkContext fosse a proprio notebook\n",
    "# spark.sparkContext.addFile(localfile) \n",
    "# path = SparkFiles.get(\"data.csv\")\n",
    "# df_spark = spark.read.csv(path, header=True, inferSchema=True, sep=\",\") \n",
    "\n",
    "if df_spark.isEmpty(): \n",
    "    logger.info(\"CSV # NÃO CARREGADO POR SER TABELA VAZIA #\")\n",
    "else:\n",
    "    tabelacsv = TabelaSpark(nome = \"order_details\", dataframe = df_spark)\n",
    "    tabelasspark.append(tabelacsv) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformacao e limpeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preencher_nulos(tabelas: List[TabelaSpark]) -> List[TabelaSpark]: \n",
    "    for tabela in tabelas: \n",
    "        for colum in tabela.dataframe.columns:\n",
    "            if tabela.dataframe.schema[colum].dataType in [ShortType(), IntegerType()]:\n",
    "                tabela.dataframe = tabela.dataframe.fillna({colum: 0}) \n",
    "            elif  tabela.dataframe.schema[colum].dataType == StringType(): \n",
    "                tabela.dataframe = tabela.dataframe.fillna({colum: \"missing\"})  \n",
    "            elif  tabela.dataframe.schema[colum].dataType == DateType(): \n",
    "                tabela.dataframe = tabela.dataframe.fillna({colum: \"1900-01-01\"})  \n",
    "    return tabelas\n",
    "\n",
    "tabelasspark_1 = preencher_nulos(tabelasspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_dict = {\n",
    "    \"region\": \"region_description\",\n",
    "    \"us_states\": \"state_name\",\n",
    "    \"territories\": \"territory_description\"\n",
    "}\n",
    "\n",
    "def retirar_duplicatasgeo(tabelas: List[TabelaSpark]) -> List[TabelaSpark]:\n",
    "    for tabela in tabelas:\n",
    "        if tabela.nome in geo_dict:\n",
    "            coluna = geo_dict[tabela.nome]\n",
    "            tabela.dataframe = tabela.dataframe.dropDuplicates([coluna])\n",
    "    return tabelas\n",
    "\n",
    "tabelasspark_2 = retirar_duplicatasgeo(tabelasspark_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entrou customers\n",
      "entrou orders\n",
      "entrou region\n",
      "entrou territories\n"
     ]
    }
   ],
   "source": [
    "#####Caso onde o poder computacional é insuficiente para o processamento dos dados ##############\n",
    "\n",
    "# listadfgrande = [\"customers\", \"orders\", \"region\", \"territories\"] \n",
    "# tabelasspark_3 = [tabela for tabela in tabelasspark_2 if tabela.nome not in listadfgrande] \n",
    "# tabelasspark_b = [tabela for tabela in tabelasspark_2 if tabela.nome in listadfgrande] \n",
    "# # tabelasspark_metade: List[TabelaSpark] = []\n",
    "# tabelasspark_div: List[TabelaSpark] = []\n",
    "\n",
    "\n",
    "\n",
    "# # def separacao(tabela, nome, cond): \n",
    "# #     tabelametade = tabela.limit(tabela.count() // 2)\n",
    "# #     if cond:\n",
    "# #         return TabelaSpark(nome, tabela) \n",
    "# #     else:\n",
    "# #         tabela.subtract(tabelametade)\n",
    "# #         return TabelaSpark(nome, tabela)\n",
    "\n",
    "\n",
    "# # def dfmetade(tabelas: List[TabelaSpark]) -> List[TabelaSpark]: \n",
    "# #     for tabela in tabelas: \n",
    "# #         if tabela.nome != \"region\" and tabela.nome != \"territories\":\n",
    "# #             print(tabela.nome)\n",
    "# #             tabelasspark_metade.append(separacao(tabela.dataframe, f\"{tabela.nome}1\", True))\n",
    "# #             tabelasspark_metade.append(separacao(tabela.dataframe, f\"{tabela.nome}2\", False))\n",
    "\n",
    "\n",
    "# def dfdividir(tabelas: List[TabelaSpark]) -> List[TabelaSpark]: \n",
    "#     for tabela in tabelas: \n",
    "#         print(\"entrou\", tabela.nome)\n",
    "#         partes = [tabela.dataframe.sample(False, 1.0/4).cache() for _ in range(4)]\n",
    "#         partes_exclusivas = [\n",
    "#             partes[0].subtract(partes[1]).subtract(partes[2]).subtract(partes[3]),\n",
    "#             partes[1].subtract(partes[0]).subtract(partes[2]).subtract(partes[3]),\n",
    "#             partes[2].subtract(partes[0]).subtract(partes[1]).subtract(partes[3]),\n",
    "#             partes[3].subtract(partes[0]).subtract(partes[1]).subtract(partes[2])\n",
    "#         ] \n",
    "#         tabelasspark_div.append(TabelaSpark(f\"{tabela.nome}1\", partes_exclusivas[0]))\n",
    "#         tabelasspark_div.append(TabelaSpark(f\"{tabela.nome}2\", partes_exclusivas[1]))\n",
    "#         tabelasspark_div.append(TabelaSpark(f\"{tabela.nome}3\", partes_exclusivas[2]))\n",
    "#         tabelasspark_div.append(TabelaSpark(f\"{tabela.nome}4\", partes_exclusivas[3]))\n",
    "\n",
    "\n",
    "\n",
    "# # dfmetade(tabelasspark_b)\n",
    "# dfdividir(tabelasspark_b) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+---------+\n",
      "|           DataFrame| nome_coluna|duplicata|\n",
      "+--------------------+------------+---------+\n",
      "|          categories| category_id|    false|\n",
      "|           customers| customer_id|    false|\n",
      "|employee_territories| employee_id|     true|\n",
      "|employee_territories|territory_id|    false|\n",
      "|           employees| employee_id|    false|\n",
      "|              orders|    order_id|    false|\n",
      "|              orders| customer_id|     true|\n",
      "|              orders| employee_id|     true|\n",
      "|            products|  product_id|    false|\n",
      "|            products| supplier_id|     true|\n",
      "|            products| category_id|     true|\n",
      "|              region|   region_id|    false|\n",
      "|            shippers|  shipper_id|    false|\n",
      "|           suppliers| supplier_id|    false|\n",
      "|         territories|territory_id|    false|\n",
      "|         territories|   region_id|     true|\n",
      "|           us_states|    state_id|    false|\n",
      "|       order_details|    order_id|     true|\n",
      "|       order_details|  product_id|     true|\n",
      "+--------------------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# POSSIVEL VERIFICAO\n",
    "def verificar_duplicatasid(tabelas: List[TabelaSpark]) -> List[tuple[str, str, bool]]: \n",
    "    duplicatasID = []\n",
    "    for tabela in tabelas: \n",
    "        tabelarepar = tabela.dataframe.repartition(4)\n",
    "        for colum in tabelarepar.columns: \n",
    "            if 'id' in colum.lower(): \n",
    "                duplicatas_existem = tabelarepar.select(colum).distinct().count() != tabelarepar.count()   \n",
    "                duplicatasID.append((tabela.nome, colum, duplicatas_existem))\n",
    "    return duplicatasID\n",
    "duplicatasID = verificar_duplicatasid(tabelasspark_2)\n",
    "\n",
    "schemaduplicadas = [\"DataFrame\", \"nome_coluna\", \"duplicata\"]\n",
    "dfduplicatasID = spark.createDataFrame(duplicatasID, schemaduplicadas) \n",
    "\n",
    "dfduplicatasID.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSSIVEL Validacao\n",
    "def validacao_nulos(tabelas: List[TabelaSpark]):\n",
    "    for tabela in tabelas: \n",
    "        for colum in tabela.dataframe.columns:\n",
    "            assert tabela.dataframe.repartition(4).filter(F.col(colum).isNull()).count() == 0, f\"Valores nulos em {colum} da tabela {tabela.nome}\" \n",
    "\n",
    "validacao_nulos(tabelasspark_2)\n",
    "# validacao_nulos(tabelasspark_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacao_integridade(tabelas: List[TabelaSpark]):\n",
    "    for tabela in tabelas: \n",
    "        assert tabela.dataframe.count() > 0, f\"DataFrame {tabela.nome} está vazio\" \n",
    "\n",
    "validacao_integridade(tabelasspark_2)\n",
    "# validacao_integridade(tabelasspark_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customers1 DataFrame[customer_id: string, company_name: string, contact_name: string, contact_title: string, address: string, city: string, region: string, postal_code: string, country: string, phone: string, fax: string]\n",
      "customers2 DataFrame[customer_id: string, company_name: string, contact_name: string, contact_title: string, address: string, city: string, region: string, postal_code: string, country: string, phone: string, fax: string]\n",
      "customers3 DataFrame[customer_id: string, company_name: string, contact_name: string, contact_title: string, address: string, city: string, region: string, postal_code: string, country: string, phone: string, fax: string]\n",
      "customers4 DataFrame[customer_id: string, company_name: string, contact_name: string, contact_title: string, address: string, city: string, region: string, postal_code: string, country: string, phone: string, fax: string]\n",
      "orders1 DataFrame[order_id: smallint, customer_id: string, employee_id: smallint, order_date: date, required_date: date, shipped_date: date, ship_via: smallint, freight: float, ship_name: string, ship_address: string, ship_city: string, ship_region: string, ship_postal_code: string, ship_country: string]\n",
      "orders2 DataFrame[order_id: smallint, customer_id: string, employee_id: smallint, order_date: date, required_date: date, shipped_date: date, ship_via: smallint, freight: float, ship_name: string, ship_address: string, ship_city: string, ship_region: string, ship_postal_code: string, ship_country: string]\n",
      "orders3 DataFrame[order_id: smallint, customer_id: string, employee_id: smallint, order_date: date, required_date: date, shipped_date: date, ship_via: smallint, freight: float, ship_name: string, ship_address: string, ship_city: string, ship_region: string, ship_postal_code: string, ship_country: string]\n",
      "orders4 DataFrame[order_id: smallint, customer_id: string, employee_id: smallint, order_date: date, required_date: date, shipped_date: date, ship_via: smallint, freight: float, ship_name: string, ship_address: string, ship_city: string, ship_region: string, ship_postal_code: string, ship_country: string]\n",
      "region1 DataFrame[region_id: smallint, region_description: string]\n",
      "region2 DataFrame[region_id: smallint, region_description: string]\n",
      "region3 DataFrame[region_id: smallint, region_description: string]\n",
      "region4 DataFrame[region_id: smallint, region_description: string]\n",
      "territories1 DataFrame[territory_id: string, territory_description: string, region_id: smallint]\n",
      "territories2 DataFrame[territory_id: string, territory_description: string, region_id: smallint]\n",
      "territories3 DataFrame[territory_id: string, territory_description: string, region_id: smallint]\n",
      "territories4 DataFrame[territory_id: string, territory_description: string, region_id: smallint]\n"
     ]
    }
   ],
   "source": [
    "# Conferir tabelas particionas quando nao ha poder computacional\n",
    "# for tabela in tabelasspark_div: \n",
    "#     print(tabela.nome, tabela.dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregamento para banco de Dados Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/26 18:18:58 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "url_DB2 = db_config['url_DB2'] \n",
    "for tabela in tabelasspark_3: \n",
    "    tabela.dataframe.write.jdbc(url_DB2, table=tabela.nome, mode=\"overwrite\", properties=properties) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caso poder compuntacional nao fosse suficiente, subida de reparticoes\n",
    "# for tabela in tabelasspark_div: \n",
    "#     print(tabela.nome)\n",
    "#     if \"1\" in tabela.nome:\n",
    "#         print(\"entra\")\n",
    "#         tabela.dataframe.write.jdbc(url_DB2, table=tabela.nome, mode=\"overwrite\", properties=properties) \n",
    "#     elif \"2\" in tabela.nome: \n",
    "#         tabela.dataframe.write.jdbc(url=url_DB2, table=tabela.nome, mode=\"append\", properties=properties)\n",
    "#     elif \"3\" in tabela.nome: \n",
    "#         tabela.dataframe.write.jdbc(url=url_DB2, table=tabela.nome, mode=\"append\", properties=properties)\n",
    "#     elif \"4\" in tabela.nome: \n",
    "#         tabela.dataframe.write.jdbc(url=url_DB2, table=tabela.nome, mode=\"append\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conexao com o GCP Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T15:31:25.166252Z",
     "iopub.status.busy": "2024-06-26T15:31:25.166086Z",
     "iopub.status.idle": "2024-06-26T15:31:25.168890Z",
     "shell.execute_reply": "2024-06-26T15:31:25.168187Z"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T15:31:25.171091Z",
     "iopub.status.busy": "2024-06-26T15:31:25.170923Z",
     "iopub.status.idle": "2024-06-26T15:31:25.382708Z",
     "shell.execute_reply": "2024-06-26T15:31:25.381842Z"
    }
   },
   "outputs": [],
   "source": [
    "# project_id = os.getenv('GOOGLE_PROJECT_ID')\n",
    "# dataset_id = os.getenv('BIGQUERY_DATASET_ID')\n",
    "\n",
    "# for tabela in tabelasspark: \n",
    "#      tabela.dataframe.write.format(\"bigquery\") \\\n",
    "#     .option(\"table\", f\"{project_id}.{dataset_id}.{tabela.nome}\") \\\n",
    "#     .save(mode=\"overwrite\")\n",
    "\n",
    "# logger.info(\"Fim do processo ETL\")\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
